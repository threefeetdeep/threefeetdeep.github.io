<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.75.1" />


<title>frosted glass - ThreeFeetDeep</title>
<meta property="og:title" content="frosted glass - ThreeFeetDeep">


  <link href='favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo3.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">about</a></li>
    
    <li><a href="https://github.com/threefeetdeep">github</a></li>
    
    <li><a href="/tags/">tags</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">14 min read</span>
    

    <h1 class="article-title">frosted glass</h1>

    
    <span class="article-date">2021-01-20</span>
    

    <div class="article-content">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p><br>
<img src="../../../../images/frosty_windscreen.png" alt="image_medium" />
<br></p>
<div id="please-tell-me-when-to-cover-my-car" class="section level2">
<h2>Please tell me when to cover my car</h2>
<p>Wouldn’t it be nice if there was a smartphone app that alerted me the evening before a potentially frosty morning, to prompt me to go and put the covers on the car windscreen, saving me checking the forecast each day?</p>
<p>There wasn’t so I thought I would have a go myself. I had previously written a very basic Hello World! app for Android, just to see what it involved, but this article is only about the heart of this app, the prediction model.</p>
</div>
<div id="different-ways-to-predict-frosty-mornings" class="section level2">
<h2>Different ways to predict frosty mornings</h2>
<p>Obviously, the best way is to get a weather forecast for the coming morning, and see if the forecasted conditions look right for frost to form on car windscreens.</p>
<div id="the-simplest-way-is-it-going-to-be-sub-zero" class="section level3">
<h3>The simplest way: is it going to be sub-zero?!</h3>
<p>We all know the freezing point of water is 0 degrees Celsius, so the app could just alert the user if the forecast is for a sub-zero air temperature for the next morning. As we shall see, the accuracy of this method can be very poor.</p>
</div>
</div>
<div id="i-did-it-my-way-logistic-regression-with-online-learning" class="section level2">
<h2>I did it my way: Logistic Regression with Online Learning</h2>
<p>I plumped for a logistic regression: feed in some variables from the forecast like air temperature, cloud cover, precipitation, wind speed and so one and out pops a probability of frost. If it’s over 0.5, then I should be alerted to go and cover the car. We would expect air temperature to be one of the most significant variables in our model.</p>
<div id="micro-climates-mainly-affecting-temperature" class="section level3">
<h3>Micro-climates mainly affecting temperature</h3>
<p>But what if I live on the top of a barren hill, or right by the sea or a lake, or a tarmac-heavy urban area? Then the forecast temperature for my general area might not be so representative of the immediate area where my car is. To handle this, I want the app to allow the logistic regression coefficients to be ‘tweaked’ over the days and months of using the app, so that the model more closely fits the observed frosty mornings where I live: in other words, ‘online learning’.</p>
</div>
<div id="feedback-tell-the-app-when-it-was-wrong" class="section level3">
<h3>Feedback: tell the app when it was wrong</h3>
<p>In my app, the user would have the option to tell the app if a prediction was wrong, i.e. it told me to cover the car, but it wasn’t actually frosty, or vice-versa. A wrong prediction could arise for two reasons:</p>
<ul>
<li>The forecast used to make the prediction the evening before was wrong!</li>
<li>The local climate where I live is “offset” from the general prediction for the reasons above.</li>
</ul>
<p>There is nothing we can do about the forecast accuracy, other than assume that in the long run, the ‘expected’ forecast matches the reality. For there being a fairly constant, ‘offset’ between the average forecast temperature and the locality of your car, we can account for this in the online learning by tweaking the logistic regression coefficients.</p>
</div>
</div>
<div id="collecting-the-data" class="section level2">
<h2>Collecting the data</h2>
<p>First, I needed some labeled data with which to train my logistic regression model. For this I set up a Raspberry Pi to record a weather forecast at 8pm each evening using a crontab, abd a forecast retrieved from the DarkSky website, appended to a weather log on the Pi:</p>
<pre class="bash"><code># the URL of the darksky website; hourly forecast for bagshot
darksky_url= &quot;https://api.darksky.net/forecast/**********/
51.4160,0.7540/?exclude=currently,minutely,daily,alerts,flags&amp;units=si&quot;

# crontab gets daily forecast and appends to log
0 20 * * *  date &gt;&gt; &quot;~/weather.log&quot;; curl $darksky_url &gt;&gt; &quot;~/weather.log&quot;; </code></pre>
<p>Everyday, I made a note of whether my car windscreen was frosted over, from 8th October 2020 to 20th January 2021. Over that period of <em>80</em> days through autumn and early winter, I saw <em>13</em> mornings with frost on my car. This is my data set.</p>
<div id="pruning-the-data" class="section level3">
<h3>Pruning the data</h3>
<p>Initially, I kept the forecast data for each day from midnight till 8am, but then decided I would only use the forecast values for 8am, under the working assumption that the conditions in the morning e.g at 8am, have the most bearing or influence on any frost formation than the weather at midnight. This data set is loaded below.</p>
<pre class="r"><code>library(tidyverse)
input_data = read.csv(file=&quot;../../static/data/input_data.csv&quot;, 
                          header = T,
                          sep = &quot;,&quot;,
                          colClasses=c(&quot;character&quot;,&quot;integer&quot;,&quot;integer&quot;,&quot;numeric&quot;,
                                       &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;,
                                       &quot;numeric&quot;, &quot;numeric&quot;, &quot;numeric&quot;,
                                       &quot;numeric&quot;, &quot;numeric&quot;)
                                       )
set.seed(1)
sample_n(tbl=input_data[c(1:2,4:12)], size=4)</code></pre>
<pre><code>##         date hr    te    dp   rh   cc    ws     pr     pi   pp Y
## 1 2021-01-08  8 -1.93 -3.35 0.90 0.94  1.59 1016.9 0.0000 0.00 1
## 2 2020-12-08  8  2.54  1.48 0.93 0.96  2.15 1002.7 0.1081 0.24 0
## 3 2020-10-08  8 15.21 13.03 0.87 0.99 10.70 1008.6 0.4945 0.37 0
## 4 2020-12-03  8  4.92  3.77 0.92 1.00  4.50  998.5 0.3647 0.50 0</code></pre>
<p>The columns are:</p>
<ul>
<li><code>hr</code> is the hour, 8am each morning, from the forecast taken at 8pm the night before</li>
<li><code>te</code> air temperature in degrees Celsius</li>
<li><code>dp</code> dew point in degrees Celsius</li>
<li><code>rh</code> relative humidity, 0 to 1</li>
<li><code>cc</code> cloud cover, 0 (clear) to 1 (completely overcast)</li>
<li><code>ws</code> wind speed in m/s</li>
<li><code>pr</code> air pressure, Hecto-pascals</li>
<li><code>pi</code> precipitation amount, mm per hour</li>
<li><code>pp</code> precipitation probability, 0 to 1</li>
<li><code>Y</code> target variable, 0 (it wasn’t a frosty morning), 1 (it was a frosty morning)</li>
</ul>
<p>The sample in the table above shows what we might expect. In the first row, the temperature is sub-zero, there is no rain, the wind is light, and my car was frosty, i.e. <code>Y=1</code>. In the other rows, the temperature is above zero, there was some rain, and it was more windy. Consequently, these rows were frost free (<code>Y=0</code>).</p>
</div>
</div>
<div id="baseline-predicting-frost-using-only-temperature" class="section level2">
<h2>Baseline: Predicting frost using only temperature</h2>
<p>As mentioned above, the simplest, most naive way to predict a frosty windscreen would be to use only the temperature. If below zero, predict frost, otherwise not! Mathematically, we predict Y as</p>
<p><span class="math inline">\(Y = H(te)\)</span></p>
<p>where <span class="math inline">\(H(te)\)</span> is the Heaviside step function - simply <span class="math inline">\(H(te)=1\)</span> if <span class="math inline">\(te &gt; 0\)</span>, and <span class="math inline">\(H(te)=0\)</span> if <span class="math inline">\(te &lt; 0\)</span>.</p>
<p>Let’s see what accuracy this achieves on the whole dataset. Note, this method needs no prior data for “training” or developing parameters, so we can just go ahead and use the whole lot for prediction…</p>
<pre class="r"><code>library(caret) # for confusion matrix
c = confusionMatrix(factor((input_data$te&lt;0)*1, levels=c(1,0)),
                   factor(input_data$Y, levels=c(1,0)))
c$table</code></pre>
<pre><code>##           Reference
## Prediction  1  0
##          1  2  2
##          0 11 65</code></pre>
<p>Out of the 13 actual frosty mornings, using this simplest of methods correctly predicted only two of them. In the other direction, of 67 mornings without frost, this method predicted 65 correctly.</p>
<p>The accuracy, true positive rate (sensitivity) and false positive rate (specificity) are:</p>
<pre><code>## Accuracy 
##   0.8375</code></pre>
<pre><code>## Sensitivity 
##   0.1538462</code></pre>
<pre><code>## Specificity 
##   0.9701493</code></pre>
<p>The true positive rate is dire - at this rate, our app would hardly ever tell me to put the covers on the car!</p>
</div>
<div id="can-we-do-better-with-a-logistic-regression" class="section level2">
<h2>Can we do better with a logistic regression?</h2>
<p>I am hoping that a logistic regression can do better than this! But to test this idea, we will need to split the input data in to training and testing sets, since we must fit our logistic regression to the “seen” training data, and test the effectiveness on the “unseen” test data. We’ll split the data 50/50 between test and training sets:</p>
<pre class="r"><code># split test train (until both have min three Y=1 rows)
set.seed(8)
inTrain &lt;- sample(x= 1:nrow(input_data), size=40,replace = F)
train_data &lt;- input_data[inTrain,]
test_data &lt;- input_data[-inTrain,]

# check equal-ish frosty days in each set
sum(train_data$Y==1)</code></pre>
<pre><code>## [1] 6</code></pre>
<pre class="r"><code>sum(test_data$Y==1)</code></pre>
<pre><code>## [1] 7</code></pre>
<div id="fitting-the-first-model" class="section level3">
<h3>Fitting the first model</h3>
<p>Now let’s fit a logistic regression on the training data, using R’s built in <code>glm()</code> function. The independent variables we’ll use are temperature, dew point, relative humidity, cloud cover, wind speed for now. I’m hoping these features are enough, without using the precipitation or air pressure data, as I want to keep the model fairly simple…</p>
<pre class="r"><code>log_reg_model &lt;- glm(Y ~ te + dp + rh + cc + ws,
                family=binomial(link=&#39;logit&#39;),
                data=train_data)

summary(log_reg_model)</code></pre>
<pre><code>## 
## Call:
## glm(formula = Y ~ te + dp + rh + cc + ws, family = binomial(link = &quot;logit&quot;), 
##     data = train_data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.1224  -0.4576  -0.2300  -0.0679   2.5983  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) -173.5452   152.7248  -1.136    0.256
## te            11.1731     9.5643   1.168    0.243
## dp           -11.6732     9.7466  -1.198    0.231
## rh           176.8186   154.0814   1.148    0.251
## cc            -2.7875     2.0582  -1.354    0.176
## ws            -0.1597     0.2418  -0.661    0.509
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 33.817  on 39  degrees of freedom
## Residual deviance: 22.499  on 34  degrees of freedom
## AIC: 34.499
## 
## Number of Fisher Scoring iterations: 6</code></pre>
</div>
<div id="testing-the-model-on-unseen-data-in-the-test-set" class="section level3">
<h3>Testing the model on unseen data in the test set</h3>
<pre class="r"><code>prediction &lt;- predict.glm(log_reg_model, newdata=test_data, type=&quot;response&quot;)</code></pre>
<pre><code>##           Reference
## Prediction  1  0
##          1  2  1
##          0  5 32</code></pre>
<pre><code>## Accuracy 
##     0.85</code></pre>
<pre><code>## Sensitivity 
##   0.2857143</code></pre>
<pre><code>## Specificity 
##    0.969697</code></pre>
<p>The sensitivity is still terrible: of the seven frosty mornings in the test set, this regression only predicted two of them! Specificity, or the false negative rate is still pretty good at about 97% as it was before.</p>
</div>
<div id="testing-using-all-data-with-unchanged-regression-coefficients" class="section level3">
<h3>Testing using all data with unchanged regression coefficients</h3>
<p>If we test using the whole data set, half of which set the logistic regression coefficients, half of which is unseen:</p>
<pre class="r"><code>prediction &lt;- predict.glm(log_reg_model, newdata=input_data, type=&quot;response&quot;)</code></pre>
<pre><code>##           Reference
## Prediction  1  0
##          1  6  1
##          0  7 66</code></pre>
<pre><code>## Accuracy 
##      0.9</code></pre>
<pre><code>## Sensitivity 
##   0.4615385</code></pre>
<pre><code>## Specificity 
##   0.9850746</code></pre>
<p>The true positive rate improves a little from about 29% to 46%, but this is still to poor to make use of in an app!</p>
</div>
</div>
<div id="online-learning-to-the-rescue" class="section level2">
<h2>Online Learning to the rescue?</h2>
<p>What if we use the test set to “nudge” the regression in the right direction by slightly changing the regression coefficients occasionally when we make an incorrect prediction?</p>
<p>Imagine that the forty days worth of test data arrives one day at a time, and each time, we get the change to tell the model whether the prediction was right or wrong. You would hope that pretty soon, after a few incorrect predictions the model could be altered to start predicting correctly more often. Let’s spell this out more concretely.</p>
<div id="my-first-online-learning-algorithm" class="section level3">
<h3>My first online learning algorithm</h3>
<p>We’ve trained an initial logistic regression model on some training data. It’s not great at predicting frosty mornings, but better at predicting when it won’t be!</p>
<p>Now the user starts using the app. The steps taken by the app are:</p>
<ul>
<li><p>get a forecast for 8am tomorrow morning</p></li>
<li><p>make a prediction (Frosty, Y=1 / Not Frosty, Y=0) using the current model</p></li>
<li><p>find out from the user if we predicted incorrectly (hopefully not to often!)</p></li>
<li><p>if actual was same as predicted, we can leave the current model unchanged</p></li>
<li><p>if actual was different, we can update the model parameters (i.e. <span class="math inline">\(b_0 + b_1x_1 + b_2x_2...\)</span>)</p>
<p>To do this update to each of the <span class="math inline">\(n\)</span> parameters <span class="math inline">\(b_j\)</span> with the new data pair <span class="math inline">\((x,Y)\)</span> where <span class="math inline">\(x\)</span> is the forecast data and <span class="math inline">\(Y\)</span> is the actual outcome, we use this reassignment of each parameter <span class="math inline">\(b\)</span>:</p></li>
</ul>
<p><span class="math display">\[\begin{equation}
\tag{1}
b_j := b_j - \alpha (h_b(x)- Y)x_j  \: \: \text{where }j=0,1,...,n
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\tag{2}
h_b(x) = \frac{1}{1+e^{-bx}}
\end{equation}\]</span></p>
<p><span class="math inline">\(h_b(x)\)</span> is the probability returned by the logistic regression model before the parameter update, and <span class="math inline">\(\alpha\)</span> is the learning rate, between 0 and 1. The closer to 1, the faster the model will adapt to our new data for Y. If <span class="math inline">\(\alpha\)</span> is 0, the parameters are left unchanged.</p>
</div>
<div id="online-learning-in-action" class="section level3">
<h3>Online Learning in action</h3>
<p>Let’s feed the test data in to the regression a day at a time until it makes an incorrect prediction:</p>
<pre class="r"><code>predict_until_incorrect &lt;- function(start_day) {
  incorrect &lt;- F
  day_count &lt;- start_day
  
  while (incorrect == F &amp; day_count &lt; 41) {
    prediction &lt;- predict.glm(log_reg_model, 
                              newdata=test_data[day_count,], 
                              type=&quot;response&quot;)
    # if prediction doesn&#39;t match reality, stop
    if ( (prediction &lt; 0.5 &amp; test_data[day_count,]$Y == 1) |
         (prediction &gt; 0.5 &amp; test_data[day_count,]$Y == 0)) {
        incorrect &lt;- T
    } 
    # go to next day
      day_count &lt;- day_count + 1
  }
  
  # which day did we stop on? or return 999 if end of data
  ifelse(day_count &lt; 41, day_count - 1 , 999)
}

# returns the day number of the next incorrect prediction
predict_until_incorrect(1)</code></pre>
<pre><code>## [1] 12</code></pre>
<pre class="r"><code># what was the prediction vs actual for that day?
(prediction &lt;- as.numeric(predict.glm(log_reg_model, 
                       newdata=test_data[12,],
                       type=&quot;response&quot;)))</code></pre>
<pre><code>## [1] 0.1612228</code></pre>
<pre class="r"><code>test_data[12,]$Y</code></pre>
<pre><code>## [1] 1</code></pre>
<p>So, on the twelfth day into our test set, the model made a wrong prediction of “not frosty” with probability 0.182, when it was in fact frosty (Y=1). Updating the regression parameters then:</p>
<pre class="r"><code># back up the parameters
original_params &lt;- log_reg_model$coefficients

# these are the ones we will adjust
params &lt;- original_params

# moderately low learning rate, don&#39;t want to change things dramatically...
alpha &lt;- 0.05

# It should have predicted Y=1, i.e. frosty
Y &lt;- 1
# day of incorrect prediction
x  &lt;- test_data[12,]
hbx &lt;- prediction  

# change x to include intercept and only terms of interest (te,dp,rh,cc...)
x &lt;- c(1,x[4:8])


# use Eqn 1 to update parameters
for (j in 1:length(params)) {
  params[[j]] &lt;-  params[[j]] - alpha*(hbx - Y)*x[[j]]  
}

original_params</code></pre>
<pre><code>## (Intercept)          te          dp          rh          cc          ws 
## -173.545240   11.173057  -11.673171  176.818637   -2.787514   -0.159727</code></pre>
<pre class="r"><code>params</code></pre>
<pre><code>##   (Intercept)            te            dp            rh            cc 
## -173.50330114   11.46662938  -11.51254484  176.85218858   -2.74851093 
##            ws 
##   -0.03642678</code></pre>
<pre class="r"><code># store these back in model
log_reg_model$coefficients &lt;- params</code></pre>
<p>Above, you can see the coefficients or parameters <span class="math inline">\(b\)</span> of the logistic regression have adjusted slightly.</p>
</div>
<div id="after-adjustment-would-the-12th-day-have-been-predicted-correctly" class="section level3">
<h3>After adjustment, would the 12th day have been predicted correctly?</h3>
<p>If we make a prediction again for the 12th day, in our test data set, but this time with the updated parameters, do the</p>
<pre class="r"><code>as.numeric((prediction &lt;- predict.glm(log_reg_model, 
                            newdata=test_data[12,], 
                            type=&quot;response&quot;)))</code></pre>
<pre><code>## [1] 0.8158679</code></pre>
<p>The prediction for that twelfth day is a probability of greater than 0.5, so frost would correctly be predicted. But have the updates improved matters for the remaining, as yet unseen, days ahead in the test set?</p>
<pre class="r"><code>(next_incorrect_day &lt;- predict_until_incorrect(start_day = 13))</code></pre>
<pre><code>## [1] 13</code></pre>
<pre class="r"><code>as.numeric((prediction &lt;- predict.glm(log_reg_model, 
                            newdata=test_data[next_incorrect_day,], 
                            type=&quot;response&quot;)))</code></pre>
<pre><code>## [1] 0.9000955</code></pre>
<pre class="r"><code>test_data[next_incorrect_day,]$Y</code></pre>
<pre><code>## [1] 0</code></pre>
<p>This time, we only make it till the next day, the 13th in the test set before making another incorrect prediction…</p>
</div>
</div>
<div id="test-set-with-parameter-tweaking-on-each-incorrect-prediction" class="section level2">
<h2>Test Set with parameter tweaking on each incorrect prediction</h2>
<p>There are forty days in the test set, let’s just go ahead and run some predictions on the whole test set using the parameter updating each time we make an incorrect prediction, and see how the model performs!</p>
<pre class="r"><code># reset the regression model back to the original &#39;untweaked&#39; parameters
log_reg_model$coefficients &lt;- original_params

# function to return predicted prob of frost on given day
get_prediction &lt;- function(day) {
   as.numeric(predict.glm(log_reg_model, 
                         newdata=test_data[day,],
                         type=&quot;response&quot;))
}

# function to perform the parameter updates
update_params &lt;- function(wrongly_predicted_day, alpha=0.05) {
  x &lt;- test_data[wrongly_predicted_day,]
  Y &lt;- test_data[wrongly_predicted_day,]$Y
  hbx &lt;- get_prediction(day_count)
  
  # change x to include intercept and only terms of interest (te,dp,rh,cc,ws)
  x &lt;- c(1,x[4:8])
  
  # update parameters
  params &lt;- log_reg_model$coefficients
  
  for (j in 1:length(params)) {
    params[[j]] &lt;-  params[[j]] - alpha*(hbx - Y)*x[[j]]  
  }
  return (params)
}


# loop through test set, updating parameters each time we make a wrong prediction
day_count &lt;- 1

while (day_count &lt; 41) {
  day_count &lt;- predict_until_incorrect(day_count)
  if (day_count  &lt; 41) {
    cat(&quot;incorrect prediction at day &quot;, day_count,
        &quot;; truth=&quot;,test_data[day_count,]$Y,
        &quot;, predicted=&quot;, get_prediction(day_count), &quot;\n&quot;) 
    log_reg_model$coefficients &lt;- update_params(day_count, alpha = 0.05)
  }
  day_count &lt;- day_count + 1
}</code></pre>
<pre><code>## incorrect prediction at day  12 ; truth= 1 , predicted= 0.1612228 
## incorrect prediction at day  13 ; truth= 0 , predicted= 0.9000955 
## incorrect prediction at day  29 ; truth= 1 , predicted= 0.2216643 
## incorrect prediction at day  30 ; truth= 0 , predicted= 0.7257951 
## incorrect prediction at day  33 ; truth= 1 , predicted= 0.1468388 
## incorrect prediction at day  34 ; truth= 0 , predicted= 0.5469925 
## incorrect prediction at day  38 ; truth= 1 , predicted= 0.091039</code></pre>
<p>We see that over the 40 days in the test set, seven errors were made, four of which were “false negatives”, i.e failing to alert the user to cover their car when it did turn out to be frosty.</p>
<p>Sadly, this means out of the seven frosty days in the test set, only three were correctly predicted, giving a slight increase over the test set from 28.6% to 42.8% true positive rate. This disappointingly turns out to be true for any <span class="math inline">\(\alpha\)</span> greater than about 0.03 used in the update loop above!</p>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<p>It remains to be seen how much more the model predictions would improve, if at all, with more real-world observations fed in to the update loop. As a user, I would get pretty annoyed if more than say 1 in 10 predictions were incorrect either way, i.e. anything less than 90% specificity and sensitivity, which we have achieved for specificity, but are a long way off for sensitivity!</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

